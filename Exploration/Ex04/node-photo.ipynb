{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":10628127,"datasetId":6580468}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 라이브러리 불러오기","metadata":{}},{"cell_type":"code","source":"import os\n\nimport urllib\n\nimport cv2\n\nimport numpy as np\n\nfrom pixellib.semantic import semantic_segmentation\n\nfrom matplotlib import pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 이미지 불러오기","metadata":{}},{"cell_type":"code","source":"# os 모듈에 있는 getenv() 함수를 이용하여 읽고싶은 파일의 경로를 file_path에 저장\n\n# 준비한 이미지 파일의 경로를 이용하여, 이미지 파일을 읽음\n\n# cv2.imread(경로): 경로에 해당하는 이미지 파일을 읽어서 변수에 저장\n\nimg_path = os.getenv('HOME')+'/aiffel/human_segmentation/images/my_image.png'  \n\nimg_orig = cv2.imread(img_path)\n\n  \n\nprint(img_orig.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 이미지 색상 변환","metadata":{}},{"cell_type":"code","source":"\n# cv2.cvtColor(입력 이미지, 색상 변환 코드): 입력 이미지의 색상 채널을 변경\n\n# cv2.COLOR_BGR2RGB: 이미지 색상 채널을 변경 (BGR 형식을 RGB 형식으로 변경)\n\n# plt.imshow(): 저장된 데이터를 이미지의 형식으로 표시, 입력은 RGB(A) 데이터 혹은 2D 스칼라 데이터\n\n# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n\n# plt.show(): 현재 열려있는 모든 figure를 표시 (여기서 figure는 이미지, 그래프 등)\n\n# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html\n\nplt.imshow(cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB))\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 모델 다운로드 - PixelLib에서 DeepLab 다운로드 받기","metadata":{}},{"cell_type":"code","source":"# 저장할 파일 이름을 결정합니다\n\n# 1. os.getenv(x)함수는 환경 변수x의 값을 포함하는 문자열 변수를 반환합니다. model_dir 에 \"/aiffel/human_segmentation/models\" 저장\n\n# 2. #os.path.join(a, b)는 경로를 병합하여 새 경로 생성 model_file 에 \"/aiffel/aiffel/human_segmentation/models/deeplabv3_xception_tf_dim_ordering_tf_kernels.h5\" 저장\n\n# 1\n\nmodel_dir = os.getenv('HOME')+'/aiffel/human_segmentation/models'\n\n# 2\n\nmodel_file = os.path.join(model_dir, 'deeplabv3_xception_tf_dim_ordering_tf_kernels.h5')\n\n  \n\n# PixelLib가 제공하는 모델의 url입니다\n\nmodel_url = 'https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.1/deeplabv3_xception_tf_dim_ordering_tf_kernels.h5'\n\n  \n\n# 다운로드를 시작합니다\n\nurllib.request.urlretrieve(model_url, model_file) # urllib 패키지 내에 있는 request 모듈의 urlretrieve 함수를 이용해서 model_url에 있는 파일을 다운로드 해서 model_file 파일명으로 저장","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 세그멘테이션 모델 생성","metadata":{}},{"cell_type":"code","source":"model = semantic_segmentation() #PixelLib 라이브러리 에서 가져온 클래스를 가져와서 semantic segmentation을 수행하는 클래스 인스턴스를 만듬\n\n\nmodel.load_pascalvoc_model(model_file) # pascal voc에 대해 훈련된 예외 모델(model_file)을 로드하는 함수를 호출","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Pascalvoc 데이터의 라벨 종류\n- background 제외 20개\n- 사람 라벨의 인덱스는 15","metadata":{}},{"cell_type":"code","source":"#pascalvoc 데이터의 라벨종류\n\nLABEL_NAMES = [\n\n    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n\n    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n\n    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n\n]\n\nlen(LABEL_NAMES)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 모델에 이미지 입력하기\n- Pascalvoc로 사전학습 된 모델 이용하기","metadata":{}},{"cell_type":"code","source":"segvalues, output = model.segmentAsPascalvoc(img_path) # segmentAsPascalvoc()함 수 를 호출 하여 입력된 이미지를 분할, 분할 출력의 배열을 가져옴, 분할 은 pacalvoc 데이터로 학습된 모델을 이용","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- 모델의 출력값 확인하기","metadata":{}},{"cell_type":"code","source":"#segmentAsPascalvoc() 함수 를 호출하여 입력된 이미지를 분할한 뒤 나온 결과값 중 output을 matplotlib을 이용해 출력\n\nplt.imshow(output)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- 결과값 중 배열 값 출력하기","metadata":{}},{"cell_type":"code","source":"segvalues # segmentAsPascalvoc() 함수를 호출하여 입력된 이미지를 분할한 뒤 나온 결과값 중 배열값을 출력","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- 배열값에 있는 class_ids를 담겨 있는 값을 통해 pascalvoc에 담겨있는 라벨을 출력","metadata":{}},{"cell_type":"code","source":"#segvalues에 있는 class_ids를 담겨있는 값을 통해 pacalvoc에 담겨있는 라벨을 출력\nfor class_id in segvalues['class_ids']:\n    print(LABEL_NAMES[class_id])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- 컬러맵 만들기","metadata":{}},{"cell_type":"code","source":"\n# 아래 코드를 이해하지 않아도 좋습니다\n# PixelLib에서 그대로 가져온 코드입니다\n# 주목해야 할 것은 생상 코드 결과물이예요!\n\n#컬러맵 만들기 \ncolormap = np.zeros((256, 3), dtype = int)\nind = np.arange(256, dtype=int)\n\nfor shift in reversed(range(8)):\n    for channel in range(3):\n        colormap[:, channel] |= ((ind >> channel) & 1) << shift\n    ind >>= 3\n\ncolormap[:20] #생성한 20개의 컬러맵 출력","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- 사람에 해당하는 배열 출력하기","metadata":{}},{"cell_type":"code","source":"colormap[15] #컬러맵 15에 해당하는 배열 출력 (pacalvoc에 LABEL_NAMES 15번째인 사람)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- 사람 외에도 다른 물체를 찾아내고 싶다면 colormap[class_id]\n- 색상순서 변경 후 마스크 만들기\n  - output 이미지는 BRG 순서로 배치 되어 있어서 색상 순서를 변경해 주어야 한다.","metadata":{}},{"cell_type":"code","source":"seg_color = (128,128,192) # 색상순서 변경 - colormap의 배열은 RGB 순이며 output의 배열은 BGR 순서로 채널 배치가 되어 있어서\n\n# output의 픽셀 별로 색상이 seg_color와 같다면 1(True), 다르다면 0(False)이 됩니다\n# seg_color 값이 person을 값이 므로 사람이 있는 위치를 제외하고는 gray로 출력\n# cmap 값을 변경하면 다른 색상으로 확인이 가능함\nseg_map = np.all(output==seg_color, axis=-1) \nprint(seg_map.shape) \nplt.imshow(seg_map, cmap='gray')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"원래 이미지와 겹쳐 세그멘테이션 확인","metadata":{}},{"cell_type":"code","source":"# 원본이미지를 img_show에 할당한뒤 이미지 사람이 있는 위치와 배경을 분리해서 표현한 color_mask 를 만든뒤 두 이미지를 합쳐서 출력\nimg_show = img_orig.copy()\n\n# True과 False인 값을 각각 255과 0으로 바꿔줍니다\nimg_mask = seg_map.astype(np.uint8) * 255\n\n# 255와 0을 적당한 색상으로 바꿔봅니다\ncolor_mask = cv2.applyColorMap(img_mask, cv2.COLORMAP_JET)\n\n# 원본 이미지와 마스트를 적당히 합쳐봅니다\n# 0.6과 0.4는 두 이미지를 섞는 비율입니다.\nimg_show = cv2.addWeighted(img_show, 0.6, color_mask, 0.4, 0.0)\n\nplt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"배경 흐리게 하기","metadata":{}},{"cell_type":"code","source":"# (13,13)은 blurring kernel size를 뜻합니다\n# 다양하게 바꿔보세요\nimg_orig_blur = cv2.blur(img_orig, (13,13))\n\n# plt.imshow(): 저장된 데이터를 이미지의 형식으로 표시한다.\n# cv2.cvtColor(입력 이미지, 색상 변환 코드): 입력 이미지의 색상 채널을 변경\n# cv2.COLOR_BGR2RGB: 원본이 BGR 순서로 픽셀을 읽다보니\n# 이미지 색상 채널을 변경해야함 (BGR 형식을 RGB 형식으로 변경)   \nplt.imshow(cv2.cvtColor(img_orig_blur, cv2.COLOR_BGR2RGB))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"흐려진 이미지에서 배경만 추출하기","metadata":{}},{"cell_type":"code","source":"# cv2.cvtColor(입력 이미지, 색상 변환 코드): 입력 이미지의 색상 채널을 변경\n# cv2.COLOR_BGR2RGB: 원본이 BGR 순서로 픽셀을 읽다보니\n# 이미지 색상 채널을 변경해야함 (BGR 형식을 RGB 형식으로 변경) \nimg_mask_color = cv2.cvtColor(img_mask, cv2.COLOR_GRAY2BGR)\n\n# cv2.bitwise_not(): 이미지가 반전됩니다. 배경이 0 사람이 255 였으나\n# 연산을 하고 나면 배경은 255 사람은 0입니다.\nimg_bg_mask = cv2.bitwise_not(img_mask_color)\n\n# cv2.bitwise_and()을 사용하면 배경만 있는 영상을 얻을 수 있습니다.\n# 0과 어떤 수를 bitwise_and 연산을 해도 0이 되기 때문에 \n# 사람이 0인 경우에는 사람이 있던 모든 픽셀이 0이 됩니다. 결국 사람이 사라지고 배경만 남아요!\nimg_bg_blur = cv2.bitwise_and(img_orig_blur, img_bg_mask)\nplt.imshow(cv2.cvtColor(img_bg_blur, cv2.COLOR_BGR2RGB))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"배경 영상과 사람 영상 합치기","metadata":{}},{"cell_type":"code","source":"# np.where(조건, 참일때, 거짓일때)\n# 세그멘테이션 마스크가 255인 부분만 원본 이미지 값을 가지고 오고 \n# 아닌 영역은 블러된 이미지 값을 사용합니다.\nimg_concat = np.where(img_mask_color==255, img_orig, img_bg_blur)\n# plt.imshow(): 저장된 데이터를 이미지의 형식으로 표시한다.\n# cv2.cvtColor(입력 이미지, 색상 변환 코드): 입력 이미지의 색상 채널을 변경\n# cv2.COLOR_BGR2RGB: 원본이 BGR 순서로 픽셀을 읽다보니 \n# 이미지 색상 채널을 변경해야함 (BGR 형식을 RGB 형식으로 변경)\nplt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LMS에 있는 퀴즈","metadata":{}},{"cell_type":"code","source":"# Q. 이번에는 사람 부분을 블러로, 배경 부분을 원본으로 출력해볼까요?\n# 힌트 : img_mask_color 옵션을 적절히 조정해주고, img_orig, img_orig_blur 를 활용하세요.\nimg_concat = np.where(np.where(img_mask_color==0, img_orig_blur, img_orig))\nplt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}