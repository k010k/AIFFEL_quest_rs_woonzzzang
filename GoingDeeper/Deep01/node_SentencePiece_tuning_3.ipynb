{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0860fbd8",
   "metadata": {},
   "source": [
    " ### 4-5. SentencePiece 하이퍼파라미터 튜닝 및 성능 개선 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a9a480",
   "metadata": {},
   "source": [
    "#### 데이터 & SentencePiece 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d579d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                           document  label\n",
      "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
      "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
      "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
      "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
      "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
      "        id                                           document  label\n",
      "0  6270596                                                굳 ㅋ      1\n",
      "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
      "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
      "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
      "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "train_file = os.getenv('HOME') + '/aiffel/sp_tokenizer/naver_data/ratings_train.txt'\n",
    "test_file = os.getenv('HOME') + '/aiffel/sp_tokenizer/naver_data/ratings_test.txt'\n",
    "\n",
    "# 데이터 불러오기\n",
    "train_data = pd.read_csv(train_file, sep='\\t')\n",
    "test_data = pd.read_csv(test_file, sep='\\t')\n",
    "\n",
    "# 데이터 확인\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ccd49db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 학습 데이터 개수: 146183\n",
      "전처리 후 테스트 데이터 개수: 49158\n"
     ]
    }
   ],
   "source": [
    "# 중복 제거\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "\n",
    "# 특수 문자 제거 (정규 표현식 사용)\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n",
    "\n",
    "print(f\"전처리 후 학습 데이터 개수: {len(train_data)}\")\n",
    "print(f\"전처리 후 테스트 데이터 개수: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee185da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링 후 학습 데이터 개수: 145791\n",
      "필터링 후 테스트 데이터 개수: 48995\n"
     ]
    }
   ],
   "source": [
    "# 결측값(NaN) 제거\n",
    "train_data = train_data.dropna(subset=['document']).copy()\n",
    "test_data = test_data.dropna(subset=['document']).copy()\n",
    "\n",
    "# 문장 길이 필터링 (NaN 방지)\n",
    "filtered_train_data = train_data[train_data['document'].apply(lambda x: 1 <= len(str(x)) <= 140)].copy()\n",
    "filtered_test_data = test_data[test_data['document'].apply(lambda x: 1 <= len(str(x)) <= 140)].copy()\n",
    "\n",
    "print(f\"필터링 후 학습 데이터 개수: {len(filtered_train_data)}\")\n",
    "print(f\"필터링 후 테스트 데이터 개수: {len(filtered_test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33cf2b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙 진짜 짜증나네요 목소리\r\n",
      "흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나\r\n",
      "너무재밓었다그래서보는것을추천한다\r\n",
      "교도소 이야기구먼 솔직히 재미는 없다평점 조정\r\n",
      "사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 $HOME/aiffel/sp_tokenizer/data/nsmc_corpus.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a9dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 369K Feb 25 05:42 /aiffel/aiffel/sp_tokenizer/data/nsmc_spm.model\r\n",
      "-rw-r--r-- 1 root root 144K Feb 25 05:42 /aiffel/aiffel/sp_tokenizer/data/nsmc_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $HOME/aiffel/sp_tokenizer/data/nsmc_spm*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14de936",
   "metadata": {},
   "source": [
    "#### SentencePiece 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd458b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰 ID: [19, 5, 21, 1144, 0]\n",
      "토큰화된 문장: ['▁이', '▁영화', '▁정말', '▁재미있다', '!']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# 학습된 SentencePiece 모델 로드\n",
    "model_path = os.getenv('HOME') + '/aiffel/sp_tokenizer/data/nsmc_spm.model'\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.Load(model_path)\n",
    "\n",
    "# 테스트 문장\n",
    "test_sentence = \"이 영화 정말 재미있다!\"\n",
    "print(\"토큰 ID:\", s.EncodeAsIds(test_sentence))\n",
    "print(\"토큰화된 문장:\", s.EncodeAsPieces(test_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a663b",
   "metadata": {},
   "source": [
    "#### sp_tokenize() 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f324ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def sp_tokenize(s, corpus): \n",
    "    tensor = []\n",
    "\n",
    "    # 문장을 토큰 ID 리스트로 변환\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    # 단어-인덱스 매핑 생성\n",
    "    vocab_file = os.getenv('HOME') + '/aiffel/sp_tokenizer/data/nsmc_spm.vocab'\n",
    "\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]  # 단어 추출\n",
    "\n",
    "        word_index.update({word: idx})  # 단어 -> 인덱스 매핑\n",
    "        index_word.update({idx: word})  # 인덱스 -> 단어 매핑\n",
    "\n",
    "    # 패딩 적용\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, word_index, index_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe623c90",
   "metadata": {},
   "source": [
    "#### sp_tokenize() 함수 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b88a744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Tensor: [[  19    5   21 1366    0]\n",
      " [ 116  383  871    0    0]\n",
      " [ 215  431 4729    0    0]]\n",
      "Word Index 예시: [('<unk>', 0), ('<s>', 1), ('</s>', 2), ('▁', 3), ('이', 4), ('▁영화', 5), ('의', 6), ('도', 7), ('가', 8), ('는', 9)]\n",
      "Index Word 예시: [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '▁'), (4, '이'), (5, '▁영화'), (6, '의'), (7, '도'), (8, '가'), (9, '는')]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 준비\n",
    "sample_sentences = [\"이 영화 정말 재미있어요!\", \"완전 최악이야.\", \"기대 이상이었습니다.\"]\n",
    "\n",
    "# 토큰화 실행\n",
    "tokenized_tensor, word_index, index_word = sp_tokenize(s, sample_sentences)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Tokenized Tensor:\", tokenized_tensor)\n",
    "print(\"Word Index 예시:\", list(word_index.items())[:10])  # 일부 단어 출력\n",
    "print(\"Index Word 예시:\", list(index_word.items())[:10])  # 일부 단어 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d107685d",
   "metadata": {},
   "source": [
    "#### 네이버 영화 리뷰 전체 데이터셋 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a0ae866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환된 훈련 데이터 크기: (145791, 116)\n",
      "변환된 테스트 데이터 크기: (48995, 107)\n"
     ]
    }
   ],
   "source": [
    "# 훈련 및 테스트 데이터 변환\n",
    "train_tensor, train_word_index, train_index_word = sp_tokenize(s, filtered_train_data['document'].tolist())\n",
    "test_tensor, test_word_index, test_index_word = sp_tokenize(s, filtered_test_data['document'].tolist())\n",
    "\n",
    "print(\"변환된 훈련 데이터 크기:\", train_tensor.shape)\n",
    "print(\"변환된 테스트 데이터 크기:\", test_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc45fe3",
   "metadata": {},
   "source": [
    "#### 감성 분석 모델 설계\n",
    "##### Org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11633be9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         1024000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,073,473\n",
      "Trainable params: 1,073,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = 8000  # SentencePiece에서 설정한 vocab_size와 동일\n",
    "embedding_dim = 128\n",
    "hidden_units = 64 # 감정 분석 위해\n",
    "\n",
    "# 모델 설계\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True),\n",
    "    LSTM(hidden_units),\n",
    "    Dense(1, activation='sigmoid')  # 감정 분석 (이진 분류)\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 구조 확인\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88582fe9",
   "metadata": {},
   "source": [
    "#### embedding_dim 변경 실험 (256, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e964d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 실험: embedding_dim = 256\n",
      "Epoch 1/5\n",
      "2278/2278 - 673s - loss: 0.3769 - accuracy: 0.8281 - val_loss: 0.3384 - val_accuracy: 0.8497\n",
      "Epoch 2/5\n"
     ]
    }
   ],
   "source": [
    "for embedding_dim in [256, 512]:\n",
    "    print(f\"📌 실험: embedding_dim = {embedding_dim}\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=8000, output_dim=embedding_dim, mask_zero=True),\n",
    "        LSTM(64),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # 레이블 데이터 준비\n",
    "    train_labels = filtered_train_data['label'].values\n",
    "    test_labels = filtered_test_data['label'].values\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(train_tensor, train_labels, validation_data=(test_tensor, test_labels),\n",
    "              epochs=5, batch_size=64, verbose=2)\n",
    "\n",
    "    # 테스트 평가\n",
    "    test_loss, test_acc = model.evaluate(test_tensor, test_labels)\n",
    "    print(f\"📊 embedding_dim = {embedding_dim}, 테스트 정확도: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62877765",
   "metadata": {},
   "source": [
    "### 회고\n",
    "\n",
    "생각보다 전체적으로 학습 시간이 다소 소요됐다.   \n",
    "그래도 KoNLPy 비교 실험 까지는 수행했는데 Okt가 생각보다 너무 오래 걸려서 실험에서 제외했다.   \n",
    "또한 모델 파라미터 변경 실험도 굉장히 느리게 진행 되고 있다.   \n",
    "사실 시간이 좀 더 여유가 있었으면 vocab_size 변경 실험은 꼭 해보고 싶었는데 추후 여유가 있을 때 수행하고 싶다.  \n",
    "운이 좋은 건지 (?) 첫 시도에 바로 SentencePiece RNN 모델 테스트에서 바로 결과가 0.8이 넘었는데 정확도를 더 높일 수 있는 방법도 좀 생각해봐야겠다.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
