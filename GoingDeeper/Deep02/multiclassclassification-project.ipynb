{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tensorflow.keras.datasets import reuters\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T13:20:36.643832Z","iopub.execute_input":"2025-03-05T13:20:36.644041Z","iopub.status.idle":"2025-03-05T13:20:49.082382Z","shell.execute_reply.started":"2025-03-05T13:20:36.644021Z","shell.execute_reply":"2025-03-05T13:20:49.081703Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# 데이터 로드\n(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\nword_index = reuters.get_word_index()\nreverse_word_index = {value: key for key, value in word_index.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T13:20:52.316106Z","iopub.execute_input":"2025-03-05T13:20:52.316464Z","iopub.status.idle":"2025-03-05T13:20:53.100711Z","shell.execute_reply.started":"2025-03-05T13:20:52.316434Z","shell.execute_reply":"2025-03-05T13:20:53.100068Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n\u001b[1m2110848/2110848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n\u001b[1m550378/550378\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 정수 인덱스 → 텍스트 변환\ndecoded_train = [\" \".join([reverse_word_index.get(word_id - 3, \"?\") for word_id in seq]) for seq in x_train]\ndecoded_test = [\" \".join([reverse_word_index.get(word_id - 3, \"?\") for word_id in seq]) for seq in x_test]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T13:20:55.437854Z","iopub.execute_input":"2025-03-05T13:20:55.438144Z","iopub.status.idle":"2025-03-05T13:20:55.693389Z","shell.execute_reply.started":"2025-03-05T13:20:55.438121Z","shell.execute_reply":"2025-03-05T13:20:55.692403Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 실험할 vocab_size 리스트\nvocab_sizes = [5000, 10000, None]\n\n# 모델 리스트\nmodels = {\n    \"MNB\": MultinomialNB(),\n    \"CNB\": ComplementNB(),\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=0),\n    \"SVM\": SVC(kernel=\"linear\", random_state=0),\n    \"Decision Tree\": DecisionTreeClassifier(random_state=0),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=0),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=0),\n}\n\n# 결과 저장\nresults = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T13:21:01.057390Z","iopub.execute_input":"2025-03-05T13:21:01.057715Z","iopub.status.idle":"2025-03-05T13:21:01.062363Z","shell.execute_reply.started":"2025-03-05T13:21:01.057689Z","shell.execute_reply":"2025-03-05T13:21:01.061508Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"for vocab_size in vocab_sizes:\n    print(f\"\\n # Vocab Size: {vocab_size}\")\n    \n    # TF-IDF 벡터화\n    vectorizer = CountVectorizer(max_features=vocab_size)\n    X_train_counts = vectorizer.fit_transform(decoded_train)\n    X_test_counts = vectorizer.transform(decoded_test)\n\n    tfidf_transformer = TfidfTransformer()\n    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n\n    # 개별 모델 평가\n    scores = {\"Vocab Size\": vocab_size}\n    for name, model in models.items():\n        model.fit(X_train_tfidf, y_train)\n        y_pred = model.predict(X_test_tfidf)\n\n        acc = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n\n        scores[f\"{name} (Acc)\"] = acc\n        scores[f\"{name} (F1)\"] = f1\n\n        print(f\"{name}: Accuracy = {acc:.4f}, F1-Score = {f1:.4f}\")\n\n    # 보팅 분류기 (Soft Voting) - 사용자 지정 구성 적용\n    logistic = LogisticRegression(penalty='l2', random_state=0)\n    complement_nb = ComplementNB()\n    gradient_boost = GradientBoostingClassifier(random_state=0)\n\n    voting_classifier = VotingClassifier(\n        estimators=[\n            ('logistic', logistic),\n            ('complement_nb', complement_nb),\n            ('gradient_boost', gradient_boost)\n        ],\n        voting='soft'\n    )\n    voting_classifier.fit(X_train_tfidf, y_train)\n    y_pred_voting = voting_classifier.predict(X_test_tfidf)\n\n    acc_voting = accuracy_score(y_test, y_pred_voting)\n    f1_voting = f1_score(y_test, y_pred_voting, average=\"weighted\")\n\n    scores[\"Voting (Acc)\"] = acc_voting\n    scores[\"Voting (F1)\"] = f1_voting\n\n    print(f\"Voting: Accuracy = {acc_voting:.4f}, F1-Score = {f1_voting:.4f}\")\n\n    results.append(scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T13:21:04.497627Z","iopub.execute_input":"2025-03-05T13:21:04.497904Z","iopub.status.idle":"2025-03-05T15:01:25.061316Z","shell.execute_reply.started":"2025-03-05T13:21:04.497884Z","shell.execute_reply":"2025-03-05T15:01:25.060315Z"}},"outputs":[{"name":"stdout","text":"\n # Vocab Size: 5000\nMNB: Accuracy = 0.6785, F1-Score = 0.6071\nCNB: Accuracy = 0.7685, F1-Score = 0.7428\nLogistic Regression: Accuracy = 0.7983, F1-Score = 0.7755\nSVM: Accuracy = 0.8246, F1-Score = 0.8146\nDecision Tree: Accuracy = 0.6968, F1-Score = 0.6941\nRandom Forest: Accuracy = 0.7640, F1-Score = 0.7415\nGradient Boosting: Accuracy = 0.7636, F1-Score = 0.7604\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Voting: Accuracy = 0.7921, F1-Score = 0.7880\n\n # Vocab Size: 10000\nMNB: Accuracy = 0.6585, F1-Score = 0.5770\nCNB: Accuracy = 0.7711, F1-Score = 0.7457\nLogistic Regression: Accuracy = 0.7961, F1-Score = 0.7729\nSVM: Accuracy = 0.8219, F1-Score = 0.8117\nDecision Tree: Accuracy = 0.6879, F1-Score = 0.6854\nRandom Forest: Accuracy = 0.7542, F1-Score = 0.7298\nGradient Boosting: Accuracy = 0.7667, F1-Score = 0.7626\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Voting: Accuracy = 0.8010, F1-Score = 0.7958\n\n # Vocab Size: None\nMNB: Accuracy = 0.5997, F1-Score = 0.5046\nCNB: Accuracy = 0.7649, F1-Score = 0.7350\nLogistic Regression: Accuracy = 0.7916, F1-Score = 0.7670\nSVM: Accuracy = 0.8224, F1-Score = 0.8119\nDecision Tree: Accuracy = 0.7039, F1-Score = 0.6981\nRandom Forest: Accuracy = 0.7342, F1-Score = 0.7064\nGradient Boosting: Accuracy = 0.7707, F1-Score = 0.7666\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Voting: Accuracy = 0.8045, F1-Score = 0.7995\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from IPython.display import display\n\n# 데이터프레임 출력\ndisplay(df_results)\n\n# Accuracy & F1-score의 최대값을 찾는 함수\ndef find_best_model(metric_type, df):\n    metric_cols = [col for col in df.columns if metric_type in col]  # Acc 또는 F1 포함된 컬럼 찾기\n    best_idx = df[metric_cols].idxmax().iloc[0]  # 최대값이 있는 행의 인덱스 찾기\n    best_model = df[metric_cols].loc[best_idx].idxmax().replace(f\" ({metric_type})\", \"\")  # 모델명 추출\n    best_vocab = df.loc[best_idx, \"Vocab Size\"]  # 해당 모델의 vocab_size 가져오기\n    best_value = df[metric_cols].max().max()  # 최대값\n\n    return best_model, best_vocab, best_value\n\n# Accuracy 최대 모델 찾기\nbest_acc_model, best_acc_vocab, best_acc_value = find_best_model(\"Acc\", df_results)\n\n# F1-score 최대 모델 찾기\nbest_f1_model, best_f1_vocab, best_f1_value = find_best_model(\"F1\", df_results)\n\n# 결과 출력\nprint(f\" Accuracy 최대값: {best_acc_value:.4f}\")\nprint(f\"    - Model: {best_acc_model}\")\nprint(f\"    - Vocab Size: {best_acc_vocab}\")\n\nprint(f\"\\n F1-Score 최대값: {best_f1_value:.4f}\")\nprint(f\"    - Model: {best_f1_model}\")\nprint(f\"    - Vocab Size: {best_f1_vocab}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T15:41:45.920100Z","iopub.execute_input":"2025-03-05T15:41:45.920498Z","iopub.status.idle":"2025-03-05T15:41:45.946061Z","shell.execute_reply.started":"2025-03-05T15:41:45.920458Z","shell.execute_reply":"2025-03-05T15:41:45.945273Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Vocab Size  MNB (Acc)  MNB (F1)  CNB (Acc)  CNB (F1)  \\\n0      5000.0   0.678540  0.607140   0.768477  0.742802   \n1     10000.0   0.658504  0.577002   0.771149  0.745685   \n2         NaN   0.599733  0.504567   0.764915  0.735010   \n\n   Logistic Regression (Acc)  Logistic Regression (F1)  SVM (Acc)  SVM (F1)  \\\n0                   0.798308                  0.775517   0.824577  0.814561   \n1                   0.796082                  0.772925   0.821906  0.811739   \n2                   0.791630                  0.767023   0.822351  0.811871   \n\n   Decision Tree (Acc)  Decision Tree (F1)  Random Forest (Acc)  \\\n0             0.696794            0.694094             0.764025   \n1             0.687890            0.685352             0.754230   \n2             0.703918            0.698145             0.734194   \n\n   Random Forest (F1)  Gradient Boosting (Acc)  Gradient Boosting (F1)  \\\n0            0.741513                 0.763580                0.760359   \n1            0.729782                 0.766696                0.762551   \n2            0.706449                 0.770703                0.766560   \n\n   Voting (Acc)  Voting (F1)  \n0      0.792075     0.788042  \n1      0.800980     0.795808  \n2      0.804541     0.799452  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Vocab Size</th>\n      <th>MNB (Acc)</th>\n      <th>MNB (F1)</th>\n      <th>CNB (Acc)</th>\n      <th>CNB (F1)</th>\n      <th>Logistic Regression (Acc)</th>\n      <th>Logistic Regression (F1)</th>\n      <th>SVM (Acc)</th>\n      <th>SVM (F1)</th>\n      <th>Decision Tree (Acc)</th>\n      <th>Decision Tree (F1)</th>\n      <th>Random Forest (Acc)</th>\n      <th>Random Forest (F1)</th>\n      <th>Gradient Boosting (Acc)</th>\n      <th>Gradient Boosting (F1)</th>\n      <th>Voting (Acc)</th>\n      <th>Voting (F1)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5000.0</td>\n      <td>0.678540</td>\n      <td>0.607140</td>\n      <td>0.768477</td>\n      <td>0.742802</td>\n      <td>0.798308</td>\n      <td>0.775517</td>\n      <td>0.824577</td>\n      <td>0.814561</td>\n      <td>0.696794</td>\n      <td>0.694094</td>\n      <td>0.764025</td>\n      <td>0.741513</td>\n      <td>0.763580</td>\n      <td>0.760359</td>\n      <td>0.792075</td>\n      <td>0.788042</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10000.0</td>\n      <td>0.658504</td>\n      <td>0.577002</td>\n      <td>0.771149</td>\n      <td>0.745685</td>\n      <td>0.796082</td>\n      <td>0.772925</td>\n      <td>0.821906</td>\n      <td>0.811739</td>\n      <td>0.687890</td>\n      <td>0.685352</td>\n      <td>0.754230</td>\n      <td>0.729782</td>\n      <td>0.766696</td>\n      <td>0.762551</td>\n      <td>0.800980</td>\n      <td>0.795808</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>0.599733</td>\n      <td>0.504567</td>\n      <td>0.764915</td>\n      <td>0.735010</td>\n      <td>0.791630</td>\n      <td>0.767023</td>\n      <td>0.822351</td>\n      <td>0.811871</td>\n      <td>0.703918</td>\n      <td>0.698145</td>\n      <td>0.734194</td>\n      <td>0.706449</td>\n      <td>0.770703</td>\n      <td>0.766560</td>\n      <td>0.804541</td>\n      <td>0.799452</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":" Accuracy 최대값: 0.8246\n    - Model: SVM\n    - Vocab Size: 5000.0\n\n F1-Score 최대값: 0.8146\n    - Model: SVM\n    - Vocab Size: 5000.0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"실험 결과를 보면 Vocab size = 5000 일 때, SVM 을 사용할 경우 Acc와 F1-score가 모두 제일 높다.   \n이를 딥러닝 모델을 활용했을 경우와 비교해보자","metadata":{}},{"cell_type":"markdown","source":"### 데이터 준비","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport numpy as np\n\n# Vocab Size 설정\nvocab_size = 5000\n\n# 토크나이저 생성 (단어를 정수 인덱스로 변환)\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(decoded_train)\n\n# 정수 시퀀스로 변환\nX_train_seq = tokenizer.texts_to_sequences(decoded_train)\nX_test_seq = tokenizer.texts_to_sequences(decoded_test)\n\n# 패딩 적용 (최대 길이를 데이터의 95% 지점으로 설정)\nmax_length = int(np.percentile([len(seq) for seq in X_train_seq], 95))\nX_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding=\"post\")\nX_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding=\"post\")\n\n# 최종 Vocab Size 설정\nvocab_size = len(tokenizer.word_index) + 1  # +1은 패딩 토큰 포함\nprint(f\"Vocab Size (Final): {vocab_size}, Max Sequence Length: {max_length}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T15:42:10.180652Z","iopub.execute_input":"2025-03-05T15:42:10.180935Z","iopub.status.idle":"2025-03-05T15:42:11.559131Z","shell.execute_reply.started":"2025-03-05T15:42:10.180915Z","shell.execute_reply":"2025-03-05T15:42:11.558401Z"}},"outputs":[{"name":"stdout","text":"Vocab Size (Final): 28136, Max Sequence Length: 426\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### RNN 모델 학습","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, Dense, LSTM, GRU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# RNN 모델 정의\ndef create_rnn_model(vocab_size, max_length):\n    model = Sequential([\n        Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n        SimpleRNN(64, return_sequences=False),\n        Dense(64, activation=\"relu\"),\n        Dense(len(set(y_train)), activation=\"softmax\")  # 다중 클래스 분류\n    ])\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\n    return model\n\n# 모델 생성 및 학습\nrnn_model = create_rnn_model(vocab_size, max_length)\n\n# 조기 종료(EarlyStopping) 콜백 설정\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n\n# 모델 학습\nrnn_model.fit(\n    X_train_padded, y_train,\n    validation_data=(X_test_padded, y_test),\n    epochs=10, batch_size=32, verbose=1,\n    callbacks=[early_stopping]\n)\n\n# 예측 수행\ny_pred_rnn = np.argmax(rnn_model.predict(X_test_padded), axis=1)\n\n# 정확도 및 F1-score 평가\nacc_rnn = accuracy_score(y_test, y_pred_rnn)\nf1_rnn = f1_score(y_test, y_pred_rnn, average=\"weighted\")\n\nprint(f\"\\n# RNN Model 결과\")\nprint(f\"Accuracy = {acc_rnn:.4f}\")\nprint(f\"F1-Score = {f1_rnn:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T15:42:15.085598Z","iopub.execute_input":"2025-03-05T15:42:15.085875Z","iopub.status.idle":"2025-03-05T15:43:16.213698Z","shell.execute_reply.started":"2025-03-05T15:42:15.085852Z","shell.execute_reply":"2025-03-05T15:43:16.212980Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.2918 - loss: 2.8018 - val_accuracy: 0.3669 - val_loss: 2.4058\nEpoch 2/10\n\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - accuracy: 0.3563 - loss: 2.3937 - val_accuracy: 0.3713 - val_loss: 2.4010\nEpoch 3/10\n\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - accuracy: 0.3705 - loss: 2.3536 - val_accuracy: 0.3669 - val_loss: 2.4045\nEpoch 4/10\n\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - accuracy: 0.3690 - loss: 2.3427 - val_accuracy: 0.3687 - val_loss: 2.4081\nEpoch 5/10\n\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - accuracy: 0.3472 - loss: 2.4305 - val_accuracy: 0.3513 - val_loss: 2.4324\n\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n\n# RNN Model 결과\nAccuracy = 0.3713\nF1-Score = 0.2189\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"결과를 보는데 너무 낮다   \n이유가 뭘까?\n\n- 벡터화된 수치 데이터가 아닌 단순 정수 인덱스를 입력으로 사용\n- 사전 학습된 임베딩을 사용하지 않고 학습 데이터로만 임베딩을 학습했기 때문\n\n정도로 생각해 볼 수 있을 것 같다..\n\n시간적으로 부족해서 vocab size를 더 많이 실험해보지 못한 점이 좀 아쉽다.   \n내가 잘 실험을 했는지도 궁금해서 다른 사람들의 결과도 한 번 보고 싶다. ","metadata":{}}]}